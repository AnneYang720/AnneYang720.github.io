{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune\n",
    "\n",
    "## 1. fine-tuning技术\n",
    "\n",
    "经典的fine-tuning方法包括将预训练模型与少量特定任务数据一起继续训练。在这个过程中，预训练模型的权重被更新，以更好地适应任务。所需的fine-tuning量取决于预训练语料库和任务特定语料库之间的相似性。如果两者相似，可能只需要少量的fine-tuning。如果两者不相似，则可能需要更多的fine-tuning。最常用的是将除了输出层以外的所有权重“冻结”（freeze）。然后随机初始化输出层参数，再以迁移学习的方式训练。仅仅更新全连接输出层，其它层的权重不变。\n",
    "\n",
    "![](./images/finetune.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter-efficient fine-tuning技术（PEFT）\n",
    "通过仅训练一小组参数来解决传统微调技术需要大量资源的问题，这些参数可能是现有模型参数的子集或新添加的一组参数。\n",
    "\n",
    "### 蒸馏distillation\n",
    "训练一个较小的模型来模仿一个较大的预训练模型的行为。预训练模型生成“教师”预测结果，然后用于训练较小的“学生”模型。通过这样做，学生模型可以从较大模型的知识中学习，而无需存储所有参数。\n",
    "\n",
    "### 适配器训练(adapter training)\n",
    "适配器是添加到预训练模型中的小型神经网络，用于特定任务的微调。这些适配器只占原始模型大小的一小部分，这使得训练更快，内存需求更低。适配器可以针对多种任务进行训练，然后插入到预训练模型中以执行新任务。\n",
    "\n",
    "### 渐进收缩(progressive shrinking)\n",
    "在fine-tuning期间逐渐减小预训练模型的大小。从一个大模型开始，逐渐减少参数的数量，直到达到所需的性能。这种方法可以产生比从头开始训练的模型性能更好的小型模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt-tuning技术\n",
    "调整输入提示（input prompt）而非修改模型参数。\n",
    "\n",
    "### Prefix tuning（前缀调整）\n",
    "前缀调整涉及学习特定任务的连续提示，在推理过程中将其添加到输入之前。通过优化这个连续提示，模型可以适应特定任务而不修改底层模型参数，这节省了计算资源并实现了高效的精调。\n",
    "\n",
    "### P-Tuning\n",
    "P-Tuning涉及训练可学习的称为“提示记号”的参数，这些参数与输入序列连接。这些提示记号是特定于任务的，在精调过程中进行优化，使得模型可以在保持原始模型参数不变的情况下在新任务上表现良好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "LoRA的思想是将原始的权重矩阵分解为两个低秩矩阵的乘积，这样就可以大大减少参数量。其本质思想还是将复杂的问题拆解为简单的问题的组合。 \n",
    "![](./images/lora.awebp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有监督的微调\n",
    "\n",
    "![](./images/instructgpt.awebp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化模型大小\n",
    "1. 有损或无损地对模型进行压缩：比如将不重要的网络节点和边去掉，这叫作剪枝；再如将16位浮点数运算变成8位整数运算，这叫作量化。\n",
    "2. 采用更有效的学习算法。比如不是从原始数据中学习，而是跟大模型学，这叫做蒸馏。\n",
    "3. 改进网络的结构，更有效发挥硬件能力等等。\n",
    "\n",
    "## 模型压缩\n",
    "\n",
    "### 剪枝\n",
    "想要压缩网络的大小，就可以通过计算，将一些不重要的节点从图中删除掉。一般在一个训练好的大网络上，一次一次迭代地将最低显著性分数的节点去掉，这样可以让损失变得最小化。 剪枝完成后，还要用剪完的网络进行微调，使得性能更好。 如果一次剪枝之后还达不到要求，这个过程可以重复多次，直到满足对于小模型的需求为止。\n",
    "![](./images/cut.awebp)\n",
    "\n",
    "\n",
    "### 量化\n",
    "量化的算法，压缩时就是把一个区间的值都映射到一个离散值上。还原时就想办法恢复成之前的值。最极端的情况下就是二值量化，这就退化成符号函数或者是激活函数了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
