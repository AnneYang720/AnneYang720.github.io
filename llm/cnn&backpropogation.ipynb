{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 卷积神经网络及反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络\n",
    "### 卷积层\n",
    "![convolution](./images/convolution.png)\n",
    "\n",
    "### 池化层\n",
    "![max pooling](./images/pooling.png)\n",
    "\n",
    "### 全连接层\n",
    "![fully connected layer](./images/fully_connect.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵&向量偏导计算\n",
    "1. 向量对向量的偏导计算\n",
    "\n",
    "$$y=Wx \\quad \\frac{d\\vec{y}}{d\\vec{x}}=W$$\n",
    "\n",
    "$$y=xW \\quad \\frac{d\\vec{y}}{d\\vec{x}}=W^{T}$$\n",
    "\n",
    "2. 向量对矩阵的偏导计算\n",
    "\n",
    "$$y=Wx \\quad \\frac{d\\vec{y_{i}}}{dW_{i,j}}=d\\vec{x_{i}}$$\n",
    "\n",
    "\n",
    "3. 矩阵对矩阵的偏导计算\n",
    "\n",
    "$$Y=WX \\quad \\frac{d\\vec{Y_{i,:}}}{d\\vec{X_{i,:}}}=W^{T}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层\n",
    "$\\sigma$-激活函数, $C$-损失函数, $a^{l}$-第l层激活值, $z^{l}$-第l层加权和, $\\mathbf{W}^{l}$-第l层权重, $b^{l}$-第l层偏置\n",
    "### 前向传播\n",
    "$$ z^{l}=\\mathbf{W}^{l}a^{l-1}+b^{l} $$\n",
    "$$ a^{l}=\\sigma(z^{l}) $$\n",
    "$$ C=\\frac{1}{2}||a^{L}-y||^{2} $$\n",
    "\n",
    "### 反向传播\n",
    "输出层L\n",
    "$$ \\delta^{L}=\\frac{\\partial C}{\\partial a^{L}}\\frac{\\partial a^{L}}{\\partial z^{L}}=(a^{L}-y) \\cdot \\sigma^{'}(z^{L})$$\n",
    "$$ \\frac{\\partial C}{\\partial W^{L}}=\\frac{\\partial C}{\\partial z^{L}}\\frac{\\partial z^{L}}{\\partial W^{L}}=\\delta^{L}(a^{L-1})^{T} $$\n",
    "$$ \\frac{\\partial C}{\\partial b^{L}}=\\frac{\\partial C}{\\partial z^{L}}\\frac{\\partial z^{L}}{\\partial b^{L}}=\\delta^{L} $$\n",
    "隐藏层\n",
    "$$ \\delta^{l}=\\frac{\\partial C}{\\partial z^{l}}=\\frac{\\partial C}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^{l}}=\\delta^{l+1}\\frac{\\partial z^{l+1}}{\\partial z^{l}}=(W^{l+1})^{T}\\delta^{l+1} \\cdot \\sigma^{'}(z^{l}) $$\n",
    "$$ \\frac{\\partial C}{\\partial W^{l}}=\\frac{\\partial C}{\\partial z^{l}}\\frac{\\partial z^{l}}{\\partial W^{l}}=\\delta^{l}(a^{l-1})^{T} $$\n",
    "$$ \\frac{\\partial C}{\\partial b^{l}}=\\frac{\\partial C}{\\partial z^{l}}\\frac{\\partial z^{l}}{\\partial b^{l}}=\\delta^{l} $$\n",
    "\n",
    "### 参数更新\n",
    "$\\eta$-学习率，取值范围(0,1)\n",
    "$$ W^{l}=W^{l}-\\eta\\frac{\\partial C}{\\partial W^{l}} $$\n",
    "$$ b^{l}=b^{l}-\\eta\\frac{\\partial C}{\\partial b^{l}} $$\n",
    "batch更新\n",
    "$$ W^{l}=W^{l}-\\eta\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial C}{\\partial W^{l}} $$\n",
    "$$ b^{l}=b^{l}-\\eta\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial C}{\\partial b^{l}} $$\n",
    "momentum更新，$\\gamma$-动量系数，取值范围(0,1)，代表指数衰减平均\n",
    "$$ v_{t}=\\gamma v_{t-1}+\\eta\\frac{\\partial C}{\\partial W^{l}} $$\n",
    "$$ W^{l}=W^{l}-v_{t} $$\n",
    "\n",
    "\n",
    "### 其他\n",
    "#### 激活函数的作用\n",
    "拟合非线性变换，如果没有（非线性）激活函数，那么多层神经网络就是一个线性变换，无法拟合复杂的非线性函数\n",
    "\n",
    "#### 正则化 - 防止过拟合\n",
    "$$ C=C_{0}+\\frac{\\lambda}{2}||W||^{2} $$\n",
    "$\\lambda$ - 正则化系数\n",
    "\n",
    "#### 梯度爆炸与梯度消失\n",
    "\n",
    "1. 两种情况下梯度消失经常出现，一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。梯度爆炸一般出现在深层网络和权值初始化值太大的情况下。\n",
    "\n",
    "- 深层网络角度\n",
    "    - 对激活函数求导，如果大于1，层数越多，梯度越大，梯度爆炸；如果小于1，层数越多，梯度越小，梯度消失\n",
    "    - 不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢\n",
    "- 激活函数角度\n",
    "    - sigmoid、tanh的导数永远小于1，层数越多，梯度越小，梯度消失\n",
    "\n",
    "2. 解决方案\n",
    "- 使用梯度裁剪、正则：解决梯度爆炸\n",
    "- 使用ReLU、LeakyReLU、ELU等激活函数\n",
    "- 使用Batch Normalization：反向传播式子中有$W$的存在，会影响梯度的消失和爆炸，batchnorm通过对每一层的输出做scale和shift的方法，把每层神经网络的输入值拉回到均值为0方差为1的分布，使得激活输入值落在非线性函数对输入比较敏感的区域，避免梯度消失\n",
    "- 使用残差网络\n",
    "$$ y=F(z)+z $$\n",
    "$$ \\frac{\\partial loss}{\\partial z_{l}} = \\frac{\\partial loss}{\\partial z_{L}}\\frac{\\partial z_{L}}{\\partial z_{l}} = \\frac{\\partial loss}{\\partial z_{L}} (1+\\frac{\\partial }{\\partial z_{l}}\\sum_{i=l}^{L-1}F(z_{i})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
