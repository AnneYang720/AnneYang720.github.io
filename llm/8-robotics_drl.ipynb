{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Algorithms for Robotic Manipulation\n",
    "## Key Concepts and Algorithms of Reinforcement Learning\n",
    "\n",
    "### Markov Decision Process and RL\n",
    "$$ MDP = (S, A, r, T, \\gamma) $$\n",
    "- $S$ - a set of states\n",
    "- $A$ - actions\n",
    "- $r, S \\times A \\to R$ - the function specifying a reward of taking an action in a state\n",
    "- $T, S \\times A \\times S \\to R$ - the state transition function\n",
    "- $\\gamma$ - the discount factor implying that a reward obtained in the future is worth a smaller amount than an immediate reward\n",
    "\n",
    "Solving an MDP involves finding a policy that determines the optimal action for each state, with the goal of maximizing the long-term discounted expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based RL\n",
    "\n",
    "#### Q-Learning\n",
    "$$ Q(s, a) = r(s, a) + \\gamma \\max_{a} Q(s', a) $$\n",
    "\n",
    "$Q(s, a)$ is the Bellman action-value function, which estimates how good it is to take an action at a given state. Q-Learning is off-policy that learns the optimal policy directly.\n",
    "\n",
    "#### SARSA\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha \\left[ R + \\gamma Q(s', a') - Q(s, a) \\right] $$\n",
    "SARSA is on-policy. \n",
    "\n",
    "#### Deep Q-Learning (DQN)\n",
    "DQN uses a neural network to approximate the Q-values. The loss function such as the MSE loss can be:\n",
    "$$ L = \\left(Q(s, a;\\theta) - (r + \\gamma \\max_{a'} Q(s', a';\\theta)) \\right) ^ 2 $$\n",
    "\n",
    "The update rule depends on the values produced by the network itself, making convergence diffucult. To address this, the DQN algorithm introduces the use of a replay buffer and target networks. The replay buffer stores past interactions as a list of tuples, which can be sampled to update the value and policy networks. This allows the network to learn from individual tuples multiple times and reduces dependence on the current experience. The target networks are time-delayed copies of the policy and Q-networks, and their parameters are updated according to the following equations:\n",
    "\n",
    "$$ \\theta_Q' \\leftarrow \\tau \\theta_Q + (1 - \\tau) \\theta_Q' $$\n",
    "$$ \\theta_\\mu' \\leftarrow \\tau \\theta_\\mu + (1 - \\tau) \\theta_\\mu' $$\n",
    "\n",
    "where $\\theta_\\mu'$ and $\\theta_Q'$ denote the parameters of the policy and Q-networks, respectively.\n",
    "\n",
    "![](./images/dqn.webp)\n",
    "\n",
    "#### Double Deep Q-Learning (Double DQN)\n",
    "$$ Q(s,a;\\theta) = r + \\gamma Q(s', argmax_{a'} Q(s', a'; \\theta); \\theta') $$\n",
    "The main neural network, $\\theta$ determines the best next action $a'$, while the target network is used to evaluate this action and compute its Q-value. This simple change has been shown to reduce overestimations of Q-values in DQN.\n",
    "\n",
    "#### Dueling Deep Q-Learning (Dueling DQN)\n",
    "Dueling DQN improves upon traditional DQN by decomposing the Q-values into two separate components\n",
    "- the value function $V(s)$, which the expected reward for a given state\n",
    "- the advantage function $A(s, a)$, which reflects the relative advantage of taking a particular action compared to other actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Based RL\n",
    "The goal of an RL agent using a PG method is to maximize the expected reward, $J(\\pi_\\theta)=E_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$, by adjusting the policy parameters $\\theta$. A standard approach to finding the optimal policy is to use gradient ascent, in which the policy parameters are updated according to the following rule:\n",
    "$$ \\theta_{t+1} = \\theta_t + \\alpha \\nabla J(\\pi_{\\theta_t}) $$\n",
    "\n",
    "This gradient can be further expanded and reformulated as:\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) R(\\tau) \\right] $$\n",
    "\n",
    "In PG methods, the policy function, which maps states to actions, is learned explicitly and actions are selected without using a value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Policy Gradient (VPG)\n",
    "In RL, it is often more important to understand the relative advantage of a particular action, rather than its absolute effectiveness.\n",
    "$$ A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s) $$\n",
    "- $A_\\pi(s, a)$ - advantage function\n",
    "- $Q_\\pi(s, a)$ - action-value function\n",
    "- $V_\\pi(s)$ - state-value function for the policy $\\pi$\n",
    "- $\\tau$ - trajectory according to the policy $\\pi_\\theta$\n",
    "\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s, a) \\right] $$\n",
    "\n",
    "![](./images/vpg.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trust Region Policy Optimization (TRPO)\n",
    "Trust region policy optimization (TRPO) constrains the optimization of a policy to a trust region. This region is defined as the area in which local approximations of the function are accurate, and the maximum step size is determined within it. The trust region is then iteratively expanded or shrunk based on how well the new approximation performs.\n",
    "\n",
    "The policy update in TRPO is given by the following optimization problem, which uses the Kullback–Leibler (KL) divergence between the old and new policies as a measure of change:\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s, a) \\right] $$\n",
    "$$ subject to E_{\\tau \\sim \\pi_\\theta} \\left[ KL[\\pi_{\\theta_{old}}(\\cdot|s_t), \\pi_\\theta(\\cdot|s_t)] \\right] \\le \\delta $$\n",
    "\n",
    "$\\delta$ is the size of the trust region, and the KL divergence between the old and new policies must be less than $\\delta$. This optimization problem can be solved using the conjugate gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximal Policy Optimization (PPO)\n",
    "Proximal policy optimization (PPO) is an algorithm that aims to address the overhead issue of TRPO by incorporating the constraint into the objective function as a penalty:\n",
    "$$ E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s, a) \\right] \\approx \\frac{1}{T} \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s, a) $$\n",
    "\n",
    "One challenge of PPO is choosing the appropriate value for C. To address this, the algorithm updates C based on the magnitude of the KL divergence. If the KL divergence is too high, C is increased, and, if it is too low, C is decreased. This allows for effective optimization over the course of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor–Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Engineering\n",
    "### Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Reinforcement Learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
