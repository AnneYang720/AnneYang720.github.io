# 神经网络
## 代价函数
$$h_\theta\left(x\right)\in \mathbb{R}^{K}$$

$\newcommand{\subk}[1]{ #1_k }$

$$J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log \subk{(h_\Theta(x^{(i)}))} + \left( 1 - y_k^{(i)} \right) \log \left( 1- \subk{\left( h_\Theta \left( x^{(i)} \right) \right)} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left( \Theta_{ji}^{(l)} \right)^2$$

## 反向传播
代价函数的偏导数 $D_{ij}^{(l)}=\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)$

$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ ${if}\; j \neq  0$

$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$ ${if}\; j = 0$

其中：

$\Delta_{ij}^{(l)}=\Delta_{ij}^{(l)}+a_{j}^{(l)}\delta_{i}^{l+1}$

$\delta^{(l)}=\left({\Theta^{(l)}}\right)^{T}\delta^{(l+1)}\ast g'\left(z^{(l)}\right)$ 点乘

## 应用建议

### 随机初始化
参数矩阵的初始化需要随机，不可全部初始化为0

### 训练技巧
1. 使用交叉验证集
2. 偏差&方差
    - 训练集误差和交叉验证集误差近似时：偏差/欠拟合
    - 交叉验证集误差远大于训练集误差时：方差/过拟合
3. 正则化与偏差&方差
    - 当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大
    - 随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加
4. 观察学习曲线
    - 在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助
    ![](./images/4a5099b9f4b6aac5785cb0ad05289335.jpg)
    - 在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果
    ![](./images/2977243994d8d28d5ff300680988ec34.jpg)
5. 不同情况下的选择
    - 获得更多的训练样本——解决高方差
    - 尝试减少特征的数量——解决高方差
    - 尝试获得更多的特征——解决高偏差
    - 尝试增加多项式特征——解决高偏差
    - 尝试减少正则化程度λ——解决高偏差
    - 尝试增加正则化程度λ——解决高方差

### 误差分析
将算法预测的结果分成四种情况：
	
1. **正确肯定**（**True Positive,TP**）：预测为真，实际为真
2. **正确否定**（**True Negative,TN**）：预测为假，实际为假
3. **错误肯定**（**False Positive,FP**）：预测为真，实际为假
4. **错误否定**（**False Negative,FN**）：预测为假，实际为真

|            |              | **预测值**    |             |
| ---------- | ------------ | ------------ | ----------- |
|            |              | **Positive** | **Negtive** |
| **实际值**  | **Positive** | **TP**       | **FN**      |
|            | **Negtive**  | **FP**       | **TN**      |

精确率Precision=**TP/(TP+FP)**

召回率Recall=**TP/(TP+FN)**

设置不同的结果阈值，两个度量值会变化，且精确率和召回率之间负相关。可以选使**F1**最高的作为阈值：

${{F}_{1}}Score:2\frac{PR}{P+R}$


# 支持向量机(Support Vector Machines)

SVM的优化目标函数
![](./images/cc66af7cbd88183efc07c8ddf09cbc73.png)

$C=1/\lambda$（类比逻辑回归），因此：
- $C$ 较大时，相当于 $\lambda$ 较小，可能会导致过拟合，高方差
- $C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差

SVM本质上还是“浅度学习”，不带核函数的SVM只能线性分类。带核函数也本质上是把线性不可分问题映射到一个可分问题上，本质还是线性分类。然而这个映射函数的构造并不容易，多数问题即使映射到高维它还是线性不可分。SVM本质上只有一层，所以是“浅度学习”。



# 无监督学习

## 聚类(Clustering) - K-Means

$μ^1$,$μ^2$,...,$μ^k$ 来表示聚类中心，用$c^{(1)}$,$c^{(2)}$,...,$c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，期代价函数（又称**畸变函数** **Distortion function**）为：

$$J(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\dfrac {1}{m}\sum^{m}_{i=1}\left\| X^{\left( i\right) }-\mu_{c^{(i)}}\right\| ^{2}$$


## 降维(Dimensionality Reduction)

### 降维动机
- 数据压缩：将数据从二维降至一维、将数据从三维降至二维...将1000维的特征降至100维
- 数据可视化：降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了

### 降维算法
#### 主成分分析(**PCA**)
主成分分析问题的描述：是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,...,$u^{(k)}$使得总的投射误差最小。

主成分的数量：在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值

错误的主要成分分析情况：将其用于减少过拟合（减少了特征的数量），这样做非常不好，不如尝试正则化处理，原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。而正则化处理时会考虑到结果变量，不会丢掉重要的数据。另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，虽然很多时候有效果，但最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）考虑采用主要成分分析。


## 异常检测(Anomaly Detection)

异常检测算法：对于给定的数据集 $x^{(1)},x^{(2)},...,x^{(m)}$，我们要针对每一个特征计算 $\mu$ 和 $\sigma^2$ 的估计值。

$\mu_j=\frac{1}{m}\sum\limits_{i=1}^{m}x_j^{(i)}$

$\sigma_j^2=\frac{1}{m}\sum\limits_{i=1}^m(x_j^{(i)}-\mu_j)^2$

一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：

$p(x)=\prod\limits_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod\limits_{j=1}^1\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$

将$p(x) = \varepsilon$作为我们的判定边界，当$p(x) > \varepsilon$时预测数据为正常数据，否则为异常。



# 大规模机器学习(Large Scale Machine Learning)

## 随机梯度下降法(Stochastic Gradient Descent)
随机梯度下降算法在每一次计算之后便更新参数 ${{\theta }}$ ，而不需要首先将所有的训练集求和

## 小批量梯度下降(Mini-Batch Gradient Descent)
小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数  ${{\theta }}$ 。

## 随机梯度下降收敛(Stochastic Gradient Descent Convergence)
随机梯度下降算法的调试，以及学习率 $α$ 的选取。

## 在线学习(Online Learning)
在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。
 Repeat forever (as long as the website is running) {
  Get $\left(x,y\right)$ corresponding to the current user 
​        $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}\right)-{y} \right){{x}_{j}}$
​       (**for** $j=0:n$) 
    }