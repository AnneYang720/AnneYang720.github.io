{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 深度学习基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络\n",
    "### 卷积层\n",
    "![convolution](./images/convolution.png)\n",
    "\n",
    "### 池化层\n",
    "![max pooling](./images/pooling.png)\n",
    "\n",
    "### 全连接层\n",
    "![fully connected layer](./images/fully_connect.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵&向量偏导计算\n",
    "1. 向量对向量的偏导计算\n",
    "\n",
    "$$y=Wx \\quad \\frac{d\\vec{y}}{d\\vec{x}}=W$$\n",
    "\n",
    "$$y=xW \\quad \\frac{d\\vec{y}}{d\\vec{x}}=W^{T}$$\n",
    "\n",
    "2. 向量对矩阵的偏导计算\n",
    "\n",
    "$$y=Wx \\quad \\frac{d\\vec{y_{i}}}{dW_{i,j}}=d\\vec{x_{i}}$$\n",
    "\n",
    "\n",
    "3. 矩阵对矩阵的偏导计算\n",
    "\n",
    "$$Y=WX \\quad \\frac{d\\vec{Y_{i,:}}}{d\\vec{X_{i,:}}}=W^{T}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层\n",
    "$\\sigma$-激活函数, $C$-损失函数, $a^{l}$-第l层激活值, $z^{l}$-第l层加权和, $\\mathbf{W}^{l}$-第l层权重, $b^{l}$-第l层偏置\n",
    "### 前向传播\n",
    "$$ z^{l}=\\mathbf{W}^{l}a^{l-1}+b^{l} $$\n",
    "$$ a^{l}=\\sigma(z^{l}) $$\n",
    "$$ C=\\frac{1}{2}||a^{L}-y||^{2} $$\n",
    "\n",
    "### 反向传播\n",
    "输出层L\n",
    "$$ \\delta^{L}=\\frac{\\partial C}{\\partial a^{L}}\\frac{\\partial a^{L}}{\\partial z^{L}}=(a^{L}-y) \\cdot \\sigma^{'}(z^{L})$$\n",
    "$$ \\frac{\\partial C}{\\partial W^{L}}=\\frac{\\partial C}{\\partial z^{L}}\\frac{\\partial z^{L}}{\\partial W^{L}}=\\delta^{L}(a^{L-1})^{T} $$\n",
    "$$ \\frac{\\partial C}{\\partial b^{L}}=\\frac{\\partial C}{\\partial z^{L}}\\frac{\\partial z^{L}}{\\partial b^{L}}=\\delta^{L} $$\n",
    "隐藏层\n",
    "$$ \\delta^{l}=\\frac{\\partial C}{\\partial z^{l}}=\\frac{\\partial C}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^{l}}=\\delta^{l+1}\\frac{\\partial z^{l+1}}{\\partial z^{l}}=(W^{l+1})^{T}\\delta^{l+1} \\cdot \\sigma^{'}(z^{l}) $$\n",
    "$$ \\frac{\\partial C}{\\partial W^{l}}=\\frac{\\partial C}{\\partial z^{l}}\\frac{\\partial z^{l}}{\\partial W^{l}}=\\delta^{l}(a^{l-1})^{T} $$\n",
    "$$ \\frac{\\partial C}{\\partial b^{l}}=\\frac{\\partial C}{\\partial z^{l}}\\frac{\\partial z^{l}}{\\partial b^{l}}=\\delta^{l} $$\n",
    "\n",
    "### 参数更新\n",
    "$\\eta$-学习率，取值范围(0,1)\n",
    "$$ W^{l}=W^{l}-\\eta\\frac{\\partial C}{\\partial W^{l}} $$\n",
    "$$ b^{l}=b^{l}-\\eta\\frac{\\partial C}{\\partial b^{l}} $$\n",
    "batch更新\n",
    "$$ W^{l}=W^{l}-\\eta\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial C}{\\partial W^{l}} $$\n",
    "$$ b^{l}=b^{l}-\\eta\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial C}{\\partial b^{l}} $$\n",
    "momentum更新，$\\gamma$-动量系数，取值范围(0,1)，代表指数衰减平均\n",
    "$$ v_{t}=\\gamma v_{t-1}+\\eta\\frac{\\partial C}{\\partial W^{l}} $$\n",
    "$$ W^{l}=W^{l}-v_{t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他\n",
    "#### 损失函数\n",
    "- 均方损失\n",
    "$$ C=\\frac{1}{2}||a^{L}-y||^{2} $$\n",
    "- 交叉熵 cross entropy\n",
    "$$ C=-\\sum_{i}y_{i}log(a_{i}^{L}) $$\n",
    "多分类问题时，均方损失函数则无法区分不同类别的样本，使用交叉熵损失函数可以更好地衡量模型预测结果与真实结果之间的差距\n",
    "- hinge loss \n",
    "$$ C=\\max(0,1-y_{i}a_{i}^{L}) $$\n",
    "- focal loss \n",
    "$$ C=-(1-a_{i}^{L})^{\\gamma}log(a_{i}^{L}) $$\n",
    "\n",
    "#### 激活函数\n",
    "激活函数的作用：拟合非线性变换，如果没有（非线性）激活函数，那么多层神经网络就是一个线性变换，无法拟合复杂的非线性函数\n",
    "1. sigmoid\n",
    "- 优点：输出值范围(0,1)，适合用于输出层；梯度平滑\n",
    "- 缺点：梯度消失；不以0为中心；计算成本高\n",
    "2. tanh\n",
    "3. ReLU\n",
    "- 优点：大于0时梯度为1，不会出现梯度消失；计算成本低\n",
    "- 缺点：不以0为中心，会导致神经元死亡；输出值范围(0,正无穷)；不以0为中心\n",
    "4. softmax\n",
    "$$ a_{i}^{L}=\\frac{e^{z_{i}^{L}}}{\\sum_{j}e^{z_{j}^{L}}} $$\n",
    "将输出值转换为概率值，适合用于多分类问题的输出层\n",
    "\n",
    "#### 正则化 - 防止过拟合\n",
    "$$ C=C_{0}+\\frac{\\lambda}{2}||W||^{2} $$\n",
    "$\\lambda$ - 正则化系数\n",
    "\n",
    "#### 梯度爆炸与梯度消失\n",
    "\n",
    "1. 两种情况下梯度消失经常出现，一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。梯度爆炸一般出现在深层网络和权值初始化值太大的情况下。\n",
    "\n",
    "- 深层网络角度\n",
    "    - 对激活函数求导，如果大于1，层数越多，梯度越大，梯度爆炸；如果小于1，层数越多，梯度越小，梯度消失\n",
    "    - 不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢\n",
    "- 激活函数角度\n",
    "    - sigmoid、tanh的导数永远小于1，层数越多，梯度越小，梯度消失\n",
    "\n",
    "2. 解决方案\n",
    "- 使用梯度裁剪、正则：解决梯度爆炸\n",
    "- 使用ReLU、LeakyReLU、ELU等激活函数\n",
    "- 使用Batch Normalization：反向传播式中有$W$的存在，会影响梯度的消失和爆炸，batchnorm通过对每一层的输出做scale和shift的方法，把每层神经网络的输入值拉回到均值为0方差为1的分布，使得激活输入值落在非线性函数对输入比较敏感的区域，避免梯度消失；且它能减少对参数更新的尺度依赖，使得学习率可以设置的更大，加速收敛\n",
    "- 使用残差网络\n",
    "$$ y=F(z)+z $$\n",
    "$$ \\frac{\\partial loss}{\\partial z_{l}} = \\frac{\\partial loss}{\\partial z_{L}}\\frac{\\partial z_{L}}{\\partial z_{l}} = \\frac{\\partial loss}{\\partial z_{L}} (1+\\frac{\\partial }{\\partial z_{l}}\\sum_{i=l}^{L-1}F(z_{i})) $$\n",
    "\n",
    "#### CNN的本质\n",
    "- 局部卷积（提取局部特征）\n",
    "- 权值共享（降低训练难度）\n",
    "- Pooling（降维，将低层次组合为高层次的特征）\n",
    "- 多层次结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 手写resnet\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
