{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经典RNN结构\n",
    "\n",
    "| 网络结构 |        结构图示         | 应用场景举例                                                 |\n",
    "| -------- | :---------------------: | ------------------------------------------------------------ |\n",
    "| 1 vs N   | ![](./images/rnn-1ton.jpg)  | 1、从图像生成文字，输入为图像的特征，输出为一段句子<br />2、根据图像生成语音或音乐，输入为图像特征，输出为一段语音或音乐 |\n",
    "| N vs 1   | ![](./images/rnn-nto1.jpg)  | 1、输出一段文字，判断其所属类别<br />2、输入一个句子，判断其情感倾向<br />3、输入一段视频，判断其所属类别 |\n",
    "| N vs M   | ![](./images/rnn-ntom.jpg) | 1、机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列<br />2、文本摘要，输入文本序列，输出这段文本序列摘要<br />3、阅读理解，输入文章，输出问题答案<br />4、语音识别，输入语音序列信息，输出文字序列 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN特点\n",
    "1. RNNs主要用于处理序列数据\n",
    "2. RNNs中当前单元的输出与之前步骤输出也有关，因此称之为循环神经网络\n",
    "3. 标准的RNNs结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值\n",
    "4. 在标准的RNN结构中，隐层的神经元之间也是带有权值的，且权值共享。\n",
    "5. 理论上，RNNs能够对任何长度序列数据进行处理。但是在实践中，为了降低复杂度往往假设当前的状态只与之前某几个时刻状态相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN公式\n",
    "![](./images/rnnbp.png)\n",
    "\n",
    "1. **前向传播**\n",
    "  - 输入层到隐层\n",
    "    - $h^{(t)}=\\phi(Ux^{(t)}+Wh^{(t-1)}+b)$\n",
    "    - $\\phi()$为激活函数，一般会选择tanh函数，$b$为偏置\n",
    "  - 隐层到输出层\n",
    "    - $o^{(t)}=Vh^{(t)}+c$\n",
    "  - 模型的预测输出\n",
    "    - $\\widehat{y}^{(t)}=\\sigma(o^{(t)})$\n",
    "    - $\\sigma$为激活函数，通常RNN用于分类，故这里一般用softmax函数\n",
    "  - 损失函数\n",
    "    - $L^{(t)}=-\\sum_{i}y_i^{(t)}\\log(\\widehat{y}_i^{(t)})$\n",
    "    - $L=\\sum_{t=1}^{n}L^{(t)}$\n",
    "\n",
    "2. **反向传播**  BPTT（back-propagation through time）\n",
    "  - 输出层到隐层\n",
    "    - $\\frac{\\partial L^{(t)}}{\\partial V}=\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\cdot \\frac{\\partial o^{(t)}}{\\partial V}$\n",
    "    - $\\frac{\\partial L}{\\partial V}=\\sum_{t=1}^{n}\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\cdot \\frac{\\partial o^{(t)}}{\\partial V}$\n",
    "  - 隐层到隐层\n",
    "    - $\\frac{\\partial L}{\\partial W}=\\sum_{k=0}^{t}\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\frac{\\partial o^{(t)}}{\\partial h^{(t)}}(\\prod_{j=k+1}^{t}\\frac{\\partial h^{(j)}}{\\partial h^{(j-1)}})\\frac{\\partial h^{(k)}}{\\partial W}$\n",
    "    - $\\frac{\\partial L}{\\partial U}=\\sum_{k=0}^{t}\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\frac{\\partial o^{(t)}}{\\partial h^{(t)}}(\\prod_{j=k+1}^{t}\\frac{\\partial h^{(j)}}{\\partial h^{(j-1)}})\\frac{\\partial h^{(k)}}{\\partial U}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "重复模块的区别\n",
    "｜ 网络结构 |        结构图示         |\n",
    "| -------- | :---------------------: | \n",
    "｜ RNN | ![](./images/rnn.png)  |\n",
    "｜ LSTM | ![](./images/lstm.png)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM公式\n",
    "\n",
    "| 网络结构 |        结构图示         | 公式 |\n",
    "| -------- | :---------------------: | ---- |\n",
    "| 遗忘门 | ![](./images/lstm-f.png)  | $$f^{(t)}=\\sigma(W_{f}h^{(t-1)}+U_{f}x^{(t)}+b_{f})$$ |\n",
    "| 输入门 | ![](./images/lstm-i.png)  | $$i^{(t)}=\\sigma(W_{i}h^{(t-1)}+U_{i}x^{(t)}+b_{i})$$ $$a^{(t)}=\\tanh(W_{a}h^{(t-1)}+U_{a}x^{(t)}+b_{a})$$ |\n",
    "| 细胞状态更新 | ![](./images/lstm-c.png)  | $$c^{(t)}=f^{(t)}\\odot c^{(t-1)}+i^{(t)}\\odot {a}^{(t)}$$ |\n",
    "| 输出门 | ![](./images/lstm-o.png)  | $$o^{(t)}=\\sigma(W_{o}h^{(t-1)}+U_{o}x^{(t)}+b_{o})$$ $$h^{(t)}=o^{(t)}\\odot \\tanh(c^{(t)})$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch LSTM\n",
    "![](./images/lstm-process.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 100])\n",
      "torch.Size([400, 50])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "lstm_seq = nn.LSTM(50, 100, num_layers=1) # 输入维度50，输出维度100\n",
    "print(lstm_seq.weight_hh_l0.size())\n",
    "print(lstm_seq.weight_ih_l0.size())\n",
    "print(lstm_seq.bias_hh_l0.size())\n",
    "print(lstm_seq.bias_ih_l0.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每组参数由4个部分组成，分别是遗忘门、输入门、细胞状态更新、输出门，每个部分的参数形状是`(hidden_size*4, input_size)`：\n",
    "- `weight_hh_l0`参数代表第0层的隐藏状态到隐藏状态的权重，在公式中分别表示为$W_{f}$、$W_{i}$、$W_{a}$、$W_{o}$，隐藏层的大小是100，所以其形状是`(4*100, 100)`\n",
    "- `weight_ih_l0`参数代表第0层的输入到隐藏状态的权重，在公式中分别表示为$U_{f}$、$U_{i}$、$U_{a}$、$U_{o}$，输入的大小是50，所以其形状是`(4*100, 50)`\n",
    "- `bias_hh_l0`参数代表第0层的隐藏状态到隐藏状态的偏置\n",
    "- `bias_ih_l0`参数代表第0层的输入到隐藏状态的偏置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "\t\tsuper(MyLSTM, self).__init__()\n",
    "\t\tself.rnn = nn.LSTM(input_size, hidden_size, num_layers) # rnn\n",
    "\t\tself.reg = nn.Linear(hidden_size, output_size) # 回归\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx, _ = self.rnn(x) # (seq, batch, hidden)\n",
    "\t\ts, b, h = x.shape\n",
    "\t\tx = x.view(s*b, h) # 转换成线性层的输入格式\n",
    "\t\tx = self.reg(x)\n",
    "\t\tx = x.view(s, b, -1)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyLSTM(2, 4)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "epoch = 100\n",
    "for t in range(epoch):\n",
    "    # 前向传播\n",
    "    out = net(Variable(train_x))\n",
    "    loss = criterion(out, Variable(train_y))\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (t + 1) % 100 == 0: # 每 100 次输出结果\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(t + 1, loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
