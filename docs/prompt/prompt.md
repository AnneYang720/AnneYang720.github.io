[Prompt Engineering Guide](https://www.promptingguide.ai/)

# Introduction

## LLM Settings
- Temperature - higher temperature leads to more randomness
- Top P - only the tokens comprising the top_p probability mass are considered for responses
- Max Length
- Stop Sequences
- Frequency Penalty - this setting reduces the repetition of words in the model's response by giving tokens that appear more a higher penalty
- Presence Penalty - the penalty is the same for all repeated tokens

## Prompt Elements
- Instruction
- Context
- Input Data
- Output Indicator

# Techniques
- Zero-shot Prompting
- Few-Shot Prompting
- Chain-of-Thought Prompting
- Self-Consistency
- Generated Knowledge Prompting
- Prompt Chaining
- Tree of Thoughts (ToT)
- Retrieval Augmented Generation (RAG)
- Automatic Reasoning and Tool-use (ART)
- Automatic Prompt Engineer (APE)
- Active-Prompt
- Directional Stimulus Prompting
- PAL (Program-Aided Language Models)
- ReAct Prompting
- Multimodal CoT Prompting

# Applications
- Function Calling
- Generating Data
- Generating Synthetic Dataset for RAG
- Tackling Generated Datasets Diversity
- Generating Code
- Graduate Job Classification Case Study
- Prompt Function
