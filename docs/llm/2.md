# 第二章 大模型的能力之GPT-3

## 2.1 语言模型的适应性：从语言模型到任务模型的转化
在自然语言处理的世界中，语言模型 $p$ 是一种对token序列 $x_{1:L}$ 的分布。这样的模型能够用于评估序列，还能用于在给定提示的条件下生成完成的序列。

在这里，任务被定义为从输入映射到输出。我们使用“适应（Adaptation）”一词来指代将语言模型转化为任务模型的过程。我们主要有两种方式来进行这种适应：

- 训练（标准的有监督学习）：训练一个新模型，使其能将输入映射到输出。这可以通过创建一个新模型并利用语言模型作为特征（探针法），或者从现有的语言模型出发，根据训练实例进行更新（微调），或者在这两者之间找到平衡（轻量级的微调）。
- 提示（上下文）学习：根据对任务的描述建一个或一组提示/上下文信息，将其输入到语言模型中以获取基于该任务的生成结果：
    - 零样本学习(Zero-shot)
    - 单样本学习(One-shot)
    - 少样本学习(Few-shot)

在这个选择过程中，训练可能会因为过拟合而变得具有挑战性（如根据5个例子微调一个有1750亿参数的模型）。且提示的局限性在于我们只能利用少量的训练实例（Transformer可输入的长度具有约束，一般来讲是2048个tokens）。

对于每个任务，讨论以下几点：
- 定义：任务是什么，以及其动机？
- 适应：我们如何通过提示将任务简化为语言模型？
- 结果：与该任务的最先进模型相比，GPT-3的定量性能如何？

模型的大小和训练样本的数量都很重要。在下面的多个任务中，对于GPT-3的默认实验设置为：
- 完整的GPT-3模型（davinci），其拥有1750亿参数。
- 使用尽可能多的使用训练数据的实例进行上下文学习。

在此过程中，实验将进行消融实验，以查看模型的大小和上下文训练实例的数量是否真的重要。对于实验结果这里先做一个预告，大模型具体很不错的性能，并且上下文的数量更多总是更好。实验的任务选择如下：
- Language modeling
- Question answering
- Translation
- Arithmetic
- News article generation
- Novel tasks

### 2.1.1 Language Modeling
语言模型 $p$ 是关于词汇序列的概率分布。
假设我们有一段文本 $x_{1:L}$ ，例如：
$$
\text{𝗍𝗁𝖾 𝗆𝗈𝗎𝗌𝖾 𝖺𝗍𝖾 𝗍𝗁𝖾 𝖼𝗁𝖾𝖾𝗌𝖾}
$$

$$
p(\text{𝗍𝗁𝖾 𝗆𝗈𝗎𝗌𝖾 𝖺𝗍𝖾 𝗍𝗁𝖾 𝖼𝗁𝖾𝖾𝗌𝖾}) = p(x_{1:L}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1})
$$


困惑度（Perplexity）是一个重要的指标，是自然语言处理和语言模型中的一个重要概念，用于衡量语言模型的性能。它可以解释为模型在预测下一个词时的平均不确定性。简单来说，如果一个模型的困惑度较低，那么它在预测下一个词的时候就会更加准确。对于给定的语言模型和一个测试数据集，困惑度被定义为：

$$
P(X) = P(x_1,x_2,...,x_N)^{(-1/N)}
$$

$X=x_{1},x_{2},...,x_{N}$ 是测试集中的词序列， $N$ 是测试集中的总词数。优秀的语言模型应能准确预测测试数据中的词序列，困惑度应较低。如果语言模型经常做出错误的预测，困惑度将较高。

一个序列的联合概率取决于其长度，并且随着长度的增长，其值趋近于零，这使得困惑度变得难以追踪。直观上，我们希望对每个词标记（token）的概率 $p(x_{i}∣x_{1:i−1})$ 进行平均。这里的 $p(x_{i}∣x_{1:i−1})$ 表示给定之前的词序列 $x_{1:i−1}$ 后，下一个词 $x_{i}$ 出现的概率。这样做的目的是评估模型在处理各种词标记时的平均性能。

事实上不希望采取算术平均，因为如果给一个词标记分配了0的概率（即我们的模型认为这个词在特定的上下文中绝对不可能出现），那么在算术平均中这会造成极大的问题。因为算术平均并不会为此惩罚你，它只是简单地将所有词标记的概率加在一起，然后除以总数，因此一个非常低的概率（如0）可能会被其他较高的概率抵消。

困惑度（perplexity）采用几何平均，这就是所做的。在几何平均中，每个词标记的概率都被同等看待，并且一个极低的概率（如0）将会导致整个几何平均大幅度下降，可以更好地衡量模型在处理所有可能的词标记时的性能，特别是在处理那些模型可能会出错的情况。

```math
\text{perplexity}_p\left(x_{1: L}\right)=\exp \left(\frac{1}{L} \sum_{i=1}^L \log \frac{1}{p\left(x_i \mid x_{1: i-1}\right)}\right) \text {. }
```

困惑度可以被理解为每个标记（token）的平均"分支因子（branching factor）"。这里的“分支因子”可以理解为在每个位置，模型认为有多少种可能的词会出现。例如，若困惑度为10，那意味着每次模型在预测下一个词时，平均上会考虑10个词作为可能的选择。

这个理解与公式中的 $\log \frac{1}{p\left(x_i \mid x_{1: i-1}\right)}$ 密切相关，这个表达式代表了编码长度。我们在计算的是平均编码长度，这个长度反映了给定当前词或标记后，下一个词或标记可能的选择数量。因此，通过对平均编码长度取指数，我们可以得到可能的选择数量，这也就是"分支因子"。

如一个长度为3的二进制字符串可以编码 $2^3=8$个可能的字符串。同样，困惑度反映了模型预测下一个词时，考虑的平均可能性数。如果困惑度为8，那么对于序列中的每个词，模型会考虑8个可能的词。这个例子类似于我们的语言模型：在给定特定词或标记后，模型需要从多个可能的选项中预测下一个词或标记。如果选择的可能性多，模型的预测任务就更为复杂，相应的困惑度就会更高。

**两类错误**：语言模型可能会犯两种类型的错误，而困惑度对这两种错误的处理方式并不对称：

- 召回错误：语言模型未能正确地为某个词符分配概率值。这种情况下，困惑度是毫不留情的。例如，如果模型为词组 '𝖺𝗍𝖾' 在 '𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾' 后出现的概率预测为接近0，那么对应的困惑度值将趋近于无穷大。

$$
p({ate} \mid {the}, {mouse}) \to 0 \quad\Rightarrow\quad \text{perplexity}_p({the}, {mouse}, {ate}, {the}, {cheese}) \to \infty.
$$

- 精确度错误：语言模型为某些错误的词序列过度分配了概率值。在这种情况下，困惑度会进行适度的惩罚。给定一个语言模型 p，假设我们将一些垃圾分布 $r$ 按照概率 $ϵ$ 混入：

$$
q(x_i \mid x_{1:i-1}) = (1-\epsilon) p(x_i \mid x_{1:i-1}) + \epsilon r(x_i \mid x_{1:i-1}).
$$

那么，我们可以计算在 $q$ 下的 $x_{1:L}$ 的困惑度：

```math
\text{perplexity}_q(x_{1:L}) \le \frac{1}{1 - \epsilon} \text{perplexity}_p(x_{1:L}) \approxeq (1 + \epsilon) \text{perplexity}_p(x_{1:L}),
```

其中，最后一个近似等式在 $ϵ$ 的值较小时成立。如果我们混入5%的垃圾信息，那么困惑度只增加 5%。需要注意的是，这样生成的语言结果会非常糟糕，因为平均每 20 个词符就会生成一个无意义的词符。

测试数据集：
- Penn Tree Bank：适应性测试，将整个文本作为提示输入到GPT-3中，并评估其困惑度
- 2.1.1.2 [LAMBADA](https://arxiv.org/pdf/1606.06031.pdf)：预测句子的最后一个词
- 2.1.1.3 [HellaSwag](https://arxiv.org/pdf/1905.07830.pdf)：从一系列选择中选出最适合完成句子的选项


### 2.1.2 Question answering
- [TriviaQA](https://arxiv.org/pdf/1705.03551.pdf)：给定一问题后生成答案
- [WebQuestions](https://aclanthology.org/D13-1160.pdf)：和TriviaQA类似是问答任务
- NaturalQuestions：回答问题

### 2.1.3 Translation
标准的评估数据集比如是WMT’14和WMT’16数据集。由于存在多种可能的翻译，所以（自动）评估指标是BLEU（它捕获了n-gram重叠的概念）。

### 2.1.4 Arithmetic
GPT-3是一个语言模型（主要是英语），但我们可以在一系列更“抽象推理”的任务上评估它，以评估GPT-3作为更通用模型的性能。

### 2.1.5 News article generation
任务：给定标题和副标题，生成新闻文章。
数据集：标题/副标题取自[newser.com](https://stanford-cs324.github.io/winter2022/lectures/capabilities/newser.com)。

### 2.1.6 Novel tasks
- 使用新词：给定一个新造的词和定义，生成使用该词的句子。
- 纠正英语语法：给定一个不合语法的句子，生成其合语法的版本。

### 2.1.7 Other tasks
自原始论文以来，GPT-3已应用于许多更多的任务，包括基准数据集(Benchmark)和一次性的演示(one-off deoms)。以下是一个不详尽的列表:
**Benchmarks：**
- [SWORDS](https://arxiv.org/pdf/2106.04102.pdf)：词汇替换，目标是在句子的上下文中预测同义词。
- [Massive Multitask Language Understanding](https://arxiv.org/pdf/2009.03300.pdf)：包括数学，美国历史，计算机科学，法律等57个多选问题。
- [TruthfulQA](https://arxiv.org/pdf/2109.07958.pdf)：人类由于误解而错误回答的问答数据集。
**结果：**虽说GPT-3在这些Benchmark数据集中的表现平庸，但是考虑到我们只使用了few-shot的情况，或许不算太差。

**one-off Demos：**
- [Examples from the OpenAI website](https://beta.openai.com/examples/)
- [Examples from gpt3demo.com](https://gpt3demo.com/)
这些演示既创新又有趣，但很难判断它们的可靠性如何。

### 2.1.8 总结
- GPT-3在广泛的标准NLP基准测试和一次性任务上进行了评估。
- GPT-3可以表现得极好或者非常普通。
- 增加模型的大小和示例的数量都有助于提高性能。
- 有一些启发式的方法可以将语言模型适应到感兴趣的任务。
- 但是为什么会有这样表现，没有人知道。