{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN\n",
    "生成对抗网络(GAN, Generative adversarial network)，由 **生成器(Generator)** 和 **判别器(Discriminator)** 组成，生成器负责生成样本，判别器负责判断生成器生成的样本是否为真。生成器要尽可能迷惑判别器，而判别器要尽可能区分生成器生成的样本和真实样本。GAN的训练过程是一个博弈过程，最终的目标是生成器生成的样本和真实样本无法区分。\n",
    "\n",
    "![](./images/gan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标函数\n",
    "\n",
    "$$\n",
    "\\mathop {\\min }\\limits_G \\mathop {\\max }\\limits_D V(D,G) = {\\rm E}_{x\\sim{p_{data}(x)}}[\\log D(x)] + {\\rm E}_{z\\sim{p_z}(z)}[\\log (1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "1. 判别器的优化通过$\\mathop {\\max}\\limits_D V(D,G)$实现，$V(D,G)$为判别器的目标函数，其第一项${\\rm E}_{x\\sim{p_{data}(x)}}[\\log D(x)]$表示对于从真实数据分布 中采用的样本 ,其被判别器判定为真实样本概率的数学期望。对于真实数据分布 中采样的样本，其预测为正样本的概率当然是越接近1越好。因此希望最大化这一项。第二项${\\rm E}_{z\\sim{p_z}(z)}[\\log (1 - D(G(z)))]$表示：对于从噪声$P_z(z)​$分布当中采样得到的样本，经过生成器生成之后得到的生成图片，然后送入判别器，其预测概率的负对数的期望，这个值自然是越大越好，这个值越大， 越接近0，也就代表判别器越好。\n",
    "\n",
    "2. 生成器的优化通过$\\mathop {\\min }\\limits_G({\\mathop {\\max }\\limits_D V(D,G)})$来实现。注意，生成器的目标不是$\\mathop {\\min }\\limits_GV(D,G)$，即生成器不是最小化判别器的目标函数，而是最小化判别器目标函数的最大值，判别器目标函数的最大值代表的是真实数据分布与生成数据分布的JS散度，JS散度可以度量分布的相似性，两个分布越接近，JS散度越小。\n",
    "\n",
    "3. GAN的Loss是不断波动的，因为判别器和生成器是交替训练的，两者的优化目标是对立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他\n",
    "1. 生成式模型、判别式模型的区别\n",
    "- 生成式模型：由数据学习联合概率分布P(X,Y), 然后由P(Y|X)=P(X,Y)/P(X)求出概率分布P(Y|X)作为预测的模型。该方法表示了给定输入X与产生输出Y的生成关系\n",
    "- 判别式模型：由数据直接学习决策函数Y=f(X)或条件概率分布P(Y|X)作为预测模型，即判别模型。判别方法关心的是对于给定的输入X，应该预测什么样的输出Y。\n",
    "\n",
    "2. 生成式模型的评价方法\n",
    "- Interception Score, Mode Score: 评价生成的样本与真实样本的相似性\n",
    "- 真实样本和特征样本在特征空间的距离\n",
    "\n",
    "3. GAN的改进与优化\n",
    "- CGAN(Conditional Generative Adversarial Networks): 条件生成对抗网络\n",
    "- DCGAN(Deep Convolutional Generative Adversarial Networks): 深度卷积生成对抗网络\n",
    "- WGAN(Wasserstein Generative Adversarial Networks): Wasserstein生成对抗网络\n",
    "- LSGAN(Least Squares Generative Adversarial Networks): 最小二乘生成对抗网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.transforms as tfs\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    net = nn.Sequential(        \n",
    "            nn.Linear(784, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_dim=96):   \n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(noise_dim, 1024),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(1024, 784),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def discriminator_loss(logits_real, logits_fake): # 判别器的 loss\n",
    "    size = logits_real.shape[0]\n",
    "    true_labels = Variable(torch.ones(size, 1)).float().cuda()\n",
    "    false_labels = Variable(torch.zeros(size, 1)).float().cuda()\n",
    "    loss = bce_loss(logits_real, true_labels) + bce_loss(logits_fake, false_labels)\n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake): # 生成器的 loss  \n",
    "    size = logits_fake.shape[0]\n",
    "    true_labels = Variable(torch.ones(size, 1)).float().cuda()\n",
    "    loss = bce_loss(logits_fake, true_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 adam 来进行训练，学习率是 3e-4, beta1 是 0.5, beta2 是 0.999\n",
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=3e-4, betas=(0.5, 0.999))\n",
    "    return optimizer\n",
    "\n",
    "def train_a_gan(D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=250, \n",
    "                noise_size=96, num_epochs=10):\n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, _ in train_data:\n",
    "            bs = x.shape[0]\n",
    "            # 判别网络\n",
    "            real_data = Variable(x).view(bs, -1).cuda() # 真实数据\n",
    "            logits_real = D_net(real_data) # 判别网络得分\n",
    "            \n",
    "            sample_noise = (torch.rand(bs, noise_size) - 0.5) / 0.5 # -1 ~ 1 的均匀分布\n",
    "            g_fake_seed = Variable(sample_noise).cuda()\n",
    "            fake_images = G_net(g_fake_seed) # 生成的假的数据\n",
    "            logits_fake = D_net(fake_images) # 判别网络得分\n",
    "\n",
    "            d_total_error = discriminator_loss(logits_real, logits_fake) # 判别器的 loss\n",
    "            D_optimizer.zero_grad()\n",
    "            d_total_error.backward()\n",
    "            D_optimizer.step() # 优化判别网络\n",
    "            \n",
    "            # 生成网络\n",
    "            g_fake_seed = Variable(sample_noise).cuda()\n",
    "            fake_images = G_net(g_fake_seed) # 生成的假的数据\n",
    "\n",
    "            gen_logits_fake = D_net(fake_images)\n",
    "            g_error = generator_loss(gen_logits_fake) # 生成网络的 loss\n",
    "            G_optimizer.zero_grad()\n",
    "            g_error.backward()\n",
    "            G_optimizer.step() # 优化生成网络\n",
    "\n",
    "            if (iter_count % show_every == 0):\n",
    "                print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count, d_total_error.data[0], g_error.data[0]))\n",
    "            iter_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = discriminator().cuda()\n",
    "G = generator().cuda()\n",
    "\n",
    "D_optim = get_optimizer(D)\n",
    "G_optim = get_optimizer(G)\n",
    "\n",
    "train_a_gan(D, G, D_optim, G_optim, discriminator_loss, generator_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
