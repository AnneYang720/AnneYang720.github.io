{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "\n",
    "强化学习是一种机器学习的范式，它不同于监督学习和非监督学习。在强化学习中，学习者是一个决策代理，它必须在一个动态环境中通过试错来学习。强化学习的目标是找到一个策略，使得在特定的情境下选择哪种行动可以得到最大的回报。强化学习的一个重要特点是，它是一个延迟回报的问题，即一个行动的回报可能是在未来的某个时间点才能得到的。强化学习的一个重要应用是在机器人控制中，机器人必须在一个未知的环境中通过试错来学习。强化学习的一个重要算法是Q-learning算法，它是一种基于值函数的学习算法。在这个项目中，我们将使用Q-learning算法来训练一个机器人在一个迷宫中找到一个目标。\n",
    "\n",
    "## Q-learning算法\n",
    "$$ Q(s, a) = (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot (R(s, a) + \\gamma \\cdot \\max_{a'} Q(s', a')) $$\n",
    "s-当前状态, a-当前行动, R-当前行动的回报, s'-下一个状态, a'-下一个行动, $\\alpha$-学习率, $\\gamma$-折扣因子\n",
    "\n",
    "## deep Q-learning算法\n",
    "对于每一个状态s, 选择一个行动a, 得到一个回报r, 并且得到下一个状态s', 用一个神经网络来拟合Q(s, a)。神经网络的输入是状态s, 输出是每一个行动a的Q值。神经网络的损失函数是:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模仿学习\n",
    "相关文档\n",
    "- [模仿学习](https://zhuanlan.zhihu.com/p/25688750)\n",
    "- [模仿学习](https://zhuanlan.zhihu.com/p/363725218)\n",
    "\n",
    "传统强化学习在训练过程中需要大量的试错，在多步决策（sequential decision）中，学习器不能频繁地得到奖励，且这种基于累积奖赏及学习方式存在非常巨大的搜索空间。\n",
    "\n",
    "模仿学习（Imitation Learning）是指从示教者提供的范例中学习，提供人类专家的决策数据，每个决策包含状态和动作序列，将所有「状态-动作对」抽取出来构造新的集合。之后就可以把状态作为特征（feature），动作作为标记（label）进行分类（对于离散动作）或回归（对于连续动作）的学习从而得到最优策略模型。模型的训练目标是使模型生成的状态-动作轨迹分布和输入的轨迹分布相匹配。\n",
    "\n",
    "在简单自动驾驶任务中，状态就是指汽车摄像头所观测到的画面$o_t$，动作就是汽车的转向角度$u_t$（很多强化学习任务中$o_t$和$u_t$是可以互换的），根据人类提供的状态动作对来习得驾驶策略。这个任务也叫做行为克隆（Behavior Cloning），即作为监督学习的模仿学习。\n",
    "![](./images/cloning.webp)\n",
    "\n",
    "但训练好的策略模型执行的轨迹和训练轨迹的误差会随时间的增加而越变越大\n",
    "![](./images/cloning-error.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据增广\n",
    "针对上述的误差累积的问题，Data Augmentation（数据增强）是一个常见的方法。如NVIDIA在自动驾驶的场景下，在车的三个方向装上摄像头，observation是摄像头的图像，action是方向盘的角度。对于正面的摄像头，期望的action就是和当前方向盘的方向一致。而对于左边和右边的摄像observation，对应的action则是将对应的方向盘方向分别向右和向左偏移对应的角度，从而就获得了相对原来三倍的数据，更重要的是，有了更多拐弯的数据，这些在正常的行驶中是占少数的，从而使得这个原先更可能出现误差累积的拐弯时刻变得更robust。\n",
    "![](./images/cloning-augmentation.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGGER算法 (Dataset Aggregation)\n",
    "该方法则将研究目的从优化策略 $\\pi_{\\theta} (u_t | o_t)$ ，即令$p_{\\pi_{\\theta}}(o_t)$趋近$p_{data}(o_t)$，转移到增加训练数据上，即令样本空间更加接近真实样本空间。\n",
    "![](./images/dagger.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模仿学习的问题\n",
    "在模仿学习中，误差主要来自两个方面，一个是对于未见过的observation的泛化误差，另一个是对于已见过observation的拟合误差。对于bahaviroal cloning和DAgger而言，都是在尽可能通过增加数据，减少第一种误差。\n",
    "\n",
    "1. Non-Markovian bahavior：人类的动作不满足马尔科夫性质\n",
    "2. Multimodal behavior：有时候在一个问题中可能存在多个可行解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模仿学习的方法还有 Interaction & active learning，Inverse reinforcement learning（IRL） 等。IRL 是应用比较多的一种方法，与其它方法直接模仿人类行为不同，它是根据观测来确定奖励函数，之后的文章会具体展开。\n",
    "\n",
    "最后再说下模仿学习的局限，一是需要人类专家提供数据这点通常得花大量精力，而且采用深度网络的学习方式又需要大量的数据；二是在一些复杂困难的行为里，我们无法提供相关的行为数据，比如飞机的花式表演。另外，人类可以自身的实践里获得越来越多的数据来实现自我的一步步提升，机器应该也可以逐渐达到这个效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
